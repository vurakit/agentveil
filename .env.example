# === Vura Proxy Configuration ===

# Upstream LLM API URL
TARGET_URL=https://api.openai.com

# Proxy listen address
LISTEN_ADDR=:8080

# Redis connection (required for PII token vault)
REDIS_ADDR=localhost:6379
REDIS_PASSWORD=

# AES-256-GCM encryption key for vault (64 hex chars = 32 bytes) 
# Generate with: openssl rand -hex 32
VEIL_ENCRYPTION_KEY=

# TLS (optional)
TLS_CERT=
TLS_KEY=

# Logging
LOG_LEVEL=info

# API Authentication (optional)
# VEIL_API_KEYS=key1,key2,key3

# Rate Limiting
# VEIL_RATE_LIMIT=100        # requests per minute
# VEIL_RATE_BURST=20         # burst size

# Multi-Provider Routing (optional)
# Set to enable multi-provider mode with Anthropic + Gemini routing.
# When set, TARGET_URL is ignored and routing is controlled by the YAML file.
# VEIL_ROUTER_CONFIG=router.yaml

# Google Gemini API Key (used in router.yaml as $GOOGLE_API_KEY)
# GOOGLE_API_KEY=your-google-api-key

# Webhooks (optional)
# VEIL_WEBHOOK_URL=https://your-server.com/webhook
# VEIL_WEBHOOK_SECRET=your-hmac-secret
# VEIL_SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...
# VEIL_DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...

